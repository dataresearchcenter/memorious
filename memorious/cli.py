"""Memorious CLI - command line interface for crawler management."""

import logging
import sys
from pathlib import Path
from typing import Annotated, Optional

import typer
from anystore.functools import weakref_cache as cache
from openaleph_procrastinate.app import run_sync_worker
from rich.console import Console
from rich.table import Table

from memorious.core import init_memorious, settings
from memorious.logic.manager import CrawlerManager
from memorious.settings import VERSION
from memorious.tasks import app as procrastinate_app

log = logging.getLogger(__name__)
console = Console(stderr=True)

cli = typer.Typer(no_args_is_help=True, pretty_exceptions_enable=settings.debug)


@cache
def get_manager() -> CrawlerManager:
    """Get cached crawler manager."""
    manager = CrawlerManager()
    if settings.config_path:
        manager.load_path(str(settings.config_path))
    return manager


def get_crawler_by_name(name: str):
    """Get a crawler by name, raising an error if not found."""
    crawler = get_manager().get(name)
    if crawler is None:
        console.print(f"[red]Crawler [bold]{name}[/bold] not found.[/red]")
        raise typer.Exit(1)
    return crawler


@cli.callback(invoke_without_command=True)
def main(
    ctx: typer.Context,
    version: Annotated[
        Optional[bool], typer.Option("--version", "-v", help="Show version")
    ] = False,
    debug: Annotated[
        bool, typer.Option("--debug/--no-debug", envvar="MEMORIOUS_DEBUG")
    ] = False,
):
    """Crawler framework for documents and structured scrapers."""
    if version:
        console.print(f"memorious {VERSION}")
        raise typer.Exit()
    init_memorious()


@cli.command("run")
def run_crawler(
    crawler: Annotated[str, typer.Argument(help="Crawler name to run")],
    continue_on_error: Annotated[
        bool,
        typer.Option(
            "--continue-on-error", help="Don't stop crawler execution on error"
        ),
    ] = False,
    flush: Annotated[
        bool,
        typer.Option("--flush", help="Delete all existing data before execution"),
    ] = False,
):
    """Queue a crawler for execution via procrastinate."""
    c = get_crawler_by_name(crawler)
    if flush:
        c.flush()
    c.run(continue_on_error=continue_on_error)
    console.print(f"Crawler [bold]{c.name}[/bold] queued for execution")


@cli.command("run-file")
def run_file(
    crawler_config: Annotated[
        Path, typer.Argument(help="Path to crawler YAML config file", exists=True)
    ],
    src: Annotated[
        bool,
        typer.Option(
            "--src",
            help="Add src folder relative to crawler YAML to Python path",
        ),
    ] = False,
    continue_on_error: Annotated[
        bool,
        typer.Option(
            "--continue-on-error", help="Don't stop crawler execution on error"
        ),
    ] = False,
    flush: Annotated[
        bool,
        typer.Option("--flush", help="Delete all existing data before execution"),
    ] = False,
):
    """Queue a crawler from a YAML config file for execution."""
    manager = CrawlerManager()
    crawler = manager.load_crawler(crawler_config)
    if not crawler:
        console.print("[red]Could not load the crawler. Exiting.[/red]")
        raise typer.Exit(1)
    if src:
        src_path = crawler_config.parent / "src"
        sys.path.insert(0, str(src_path))
    if flush:
        crawler.flush()
    crawler.run(continue_on_error=continue_on_error)
    console.print(f"Crawler [bold]{crawler.name}[/bold] queued for execution")


@cli.command("worker")
def worker(
    concurrency: Annotated[
        int, typer.Option("--concurrency", "-c", help="Number of concurrent workers")
    ] = 1,
):
    """Start the procrastinate worker to process crawler jobs."""
    console.print(f"Starting memorious worker with concurrency={concurrency}")
    run_sync_worker(procrastinate_app, concurrency=concurrency)


@cli.command("cancel")
def cancel(
    crawler: Annotated[str, typer.Argument(help="Crawler name to cancel")],
):
    """Abort execution of a specified crawler."""
    c = get_crawler_by_name(crawler)
    c.cancel()
    console.print(f"Crawler [bold]{c.name}[/bold] cancelled")


@cli.command("flush")
def flush_crawler(
    crawler: Annotated[str, typer.Argument(help="Crawler name to flush")],
):
    """Delete all data generated by a crawler."""
    c = get_crawler_by_name(crawler)
    c.flush()
    console.print(f"Crawler [bold]{c.name}[/bold] data flushed")


@cli.command("flush-tags")
def flush_tags(
    crawler: Annotated[str, typer.Argument(help="Crawler name")],
):
    """Delete all tags generated by a crawler."""
    c = get_crawler_by_name(crawler)
    c.flush_tags()
    console.print(f"Crawler [bold]{c.name}[/bold] tags flushed")


@cli.command("list")
def list_crawlers():
    """List the available crawlers."""
    table = Table(title="Available Crawlers")
    table.add_column("Name", style="cyan")
    table.add_column("Description")

    for crawler in get_manager():
        table.add_row(crawler.name, crawler.description or "")

    console.print(table)


@cli.command("settings")
def show_settings():
    """Show current runtime settings."""
    console.print(settings)


if __name__ == "__main__":
    cli()

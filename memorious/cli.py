"""Memorious CLI - command line interface for crawler management."""

import logging
import sys
from pathlib import Path

import click
from anystore.functools import weakref_cache as cache
from openaleph_procrastinate.app import run_sync_worker
from tabulate import tabulate

from memorious.core import init_memorious, settings
from memorious.logic.manager import CrawlerManager
from memorious.tasks import app as procrastinate_app

log = logging.getLogger(__name__)


@cache
def get_manager() -> CrawlerManager:
    """Get cached crawler manager."""
    manager = CrawlerManager()
    if settings.config_path:
        manager.load_path(str(settings.config_path))
    return manager


@click.group()
@click.option("--debug/--no-debug", default=False, envvar="MEMORIOUS_DEBUG")
@click.option("--cache/--no-cache", default=True, envvar="MEMORIOUS_HTTP_CACHE")
@click.option(
    "--incremental/--non-incremental", default=True, envvar="MEMORIOUS_INCREMENTAL"
)
def cli(debug, cache, incremental):
    """Crawler framework for documents and structured scrapers."""
    init_memorious()


def get_crawler_by_name(name):
    crawler = get_manager().get(name)
    if crawler is None:
        msg = "Crawler [%s] not found." % name
        raise click.BadParameter(msg, param=crawler)
    return crawler


@cli.command("run")
@click.argument("crawler")
@click.option(
    "--continue-on-error",
    is_flag=True,
    default=False,
    help="Don't stop crawler execution when an error occurs",
)
@click.option(
    "--flush",
    is_flag=True,
    default=False,
    help="Delete all existing data generated by the crawler before execution",
)
def run(crawler, continue_on_error=False, flush=False):
    """Queue a crawler for execution via procrastinate."""
    crawler = get_crawler_by_name(crawler)
    if flush:
        crawler.flush()
    crawler.run(continue_on_error=continue_on_error)
    log.info("Crawler %s queued for execution", crawler.name)


@cli.command()
@click.argument(
    "crawler_config",
    type=click.Path(exists=True),
)
@click.option(
    "--src",
    required=False,
    is_flag=True,
    default=False,
    help="Load source file directory used by the crawler and add it to path."
    "Source files should be in `src` folder relative to the crawler YAML",
)
@click.option(
    "--continue-on-error",
    is_flag=True,
    default=False,
    help="Don't stop crawler execution when an error occurs",
)
@click.option(
    "--flush",
    is_flag=True,
    default=False,
    help="Delete all existing data generated by the crawler before execution",
)
def run_file(
    crawler_config,
    src=False,
    continue_on_error=False,
    flush=False,
):
    """Queue a crawler from a YAML config file for execution."""
    manager = CrawlerManager()
    crawler_config = Path(crawler_config)
    crawler = manager.load_crawler(crawler_config)
    if not crawler:
        log.warning("Could not load the crawler. Exiting.")
        return
    if src:
        src_path = crawler_config.parent / "src"
        sys.path.insert(0, str(src_path))
    if flush:
        crawler.flush()
    crawler.run(continue_on_error=continue_on_error)
    log.info("Crawler %s queued for execution", crawler.name)


@cli.command("worker")
@click.option(
    "--concurrency",
    type=int,
    default=1,
    help="Number of concurrent workers",
)
def worker(concurrency):
    """Start the procrastinate worker to process crawler jobs."""
    log.info("Starting memorious worker with concurrency=%d", concurrency)
    run_sync_worker(procrastinate_app, concurrency=concurrency)


@cli.command()
@click.argument("crawler")
def cancel(crawler):
    """Abort execution of a specified crawler."""
    crawler = get_crawler_by_name(crawler)
    crawler.cancel()


@cli.command()
@click.argument("crawler")
def flush(crawler):
    """Delete all data generated by a crawler."""
    crawler = get_crawler_by_name(crawler)
    crawler.flush()


@cli.command("flush-tags")
@click.argument("crawler")
def flush_tags(crawler):
    """Delete all tags generated by a crawler."""
    crawler = get_crawler_by_name(crawler)
    crawler.flush_tags()


@cli.command("list")
def index():
    """List the available crawlers."""
    crawler_list = []
    for crawler in get_manager():
        crawler_list.append(
            [
                crawler.name,
                crawler.description,
            ]
        )
    headers = [
        "Name",
        "Description",
    ]
    print(tabulate(crawler_list, headers=headers))


def main():
    cli(obj={})


if __name__ == "__main__":
    main()

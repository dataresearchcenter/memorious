"""Memorious CLI - command line interface for crawler management."""

from datetime import datetime, timedelta
from typing import Annotated, Optional

import typer
from anystore.logging import configure_logging, get_logger
from rich.console import Console
from rich.table import Table

from memorious.core import init_memorious, settings
from memorious.logic.crawler import get_crawler
from memorious.settings import VERSION

log = get_logger(__name__)
console = Console(stderr=True)

cli = typer.Typer(no_args_is_help=True, pretty_exceptions_enable=settings.debug)


@cli.callback(invoke_without_command=True)
def main(
    version: Annotated[
        Optional[bool], typer.Option("--version", "-v", help="Show version")
    ] = False,
    settings_: Annotated[
        Optional[bool], typer.Option("--settings", help="Show current settings")
    ] = False,
):
    """Crawler framework for documents and structured scrapers."""
    if version:
        console.print(f"memorious {VERSION}")
        raise typer.Exit()
    if settings_:
        console.print(settings)

    init_memorious()


@cli.command("run")
def run_crawler(
    uri: Annotated[str, typer.Argument(help="URI or path to crawler YAML config file")],
    continue_on_error: Annotated[
        bool,
        typer.Option(
            "--continue-on-error", help="Don't stop crawler execution on error"
        ),
    ] = False,
    flush: Annotated[
        bool,
        typer.Option("--flush", help="Delete all existing data before execution"),
    ] = False,
    concurrency: Annotated[
        int,
        typer.Option(
            "--concurrency",
            "-c",
            help="Number of concurrent jobs (use >1 for I/O-bound crawlers)",
        ),
    ] = 1,
    wait: Annotated[
        bool,
        typer.Option(
            "--wait",
            "-w",
            help="Keep worker running after jobs complete (until interrupted)",
        ),
    ] = False,
    idle_timeout: Annotated[
        int,
        typer.Option(
            "--idle-timeout",
            "-t",
            help=(
                "Auto-stop after N seconds of inactivity "
                "(default: 30 for concurrency>1, 0 to disable)"
            ),
        ),
    ] = 30,
    clear_runs: Annotated[
        bool,
        typer.Option(
            "--clear-runs/--no-clear-runs",
            help="Cancel remaining tasks from previous runs before starting",
        ),
    ] = True,
):
    """Run a crawler from a YAML config file."""
    crawler = get_crawler(uri)
    if flush:
        crawler.flush()

    # Convert 0 to None to let Crawler.run() apply defaults
    effective_idle_timeout = idle_timeout if idle_timeout > 0 else None

    crawler.log.info(
        f"[{crawler.name}] Starting Memorious crawler ...",
        continue_on_error=continue_on_error,
        flush=flush,
        concurrency=concurrency,
        wait=wait,
        idle_timeout=effective_idle_timeout,
        clear_runs=clear_runs,
    )
    crawler.run(
        continue_on_error=continue_on_error,
        concurrency=concurrency,
        wait=wait,
        idle_timeout=effective_idle_timeout,
        clear_runs=clear_runs,
    )
    crawler.log.info(f"[{crawler.name}] Crawler completed.")


@cli.command("worker")
def worker(
    concurrency: Annotated[
        int,
        typer.Option(
            "--concurrency",
            "-c",
            help="Number of concurrent jobs (use >1 for I/O-bound crawlers)",
        ),
    ] = 1,
):
    """Start the procrastinate worker to process crawler jobs."""
    from procrastinate.jobs import DeleteJobCondition

    from memorious.tasks import app as procrastinate_app

    console.print(f"Starting memorious worker with concurrency={concurrency}")
    procrastinate_app.run_worker(
        wait=True,
        concurrency=concurrency,
        delete_jobs=DeleteJobCondition.SUCCESSFUL,
    )


@cli.command("cancel")
def cancel(
    uri: Annotated[str, typer.Argument(help="URI or path to crawler YAML config file")],
):
    """Cancel pending jobs for a crawler."""
    crawler = get_crawler(uri)
    crawler.cancel()
    console.print(f"Crawler [bold]{crawler.name}[/bold] cancelled")


@cli.command("flush")
def flush_crawler(
    uri: Annotated[str, typer.Argument(help="URI or path to crawler YAML config file")],
):
    """Delete all data and tags generated by a crawler."""
    crawler = get_crawler(uri)
    crawler.flush()
    console.print(f"Crawler [bold]{crawler.name}[/bold] data flushed")


def format_age(delta: timedelta) -> str:
    """Format a timedelta as a human-readable age string."""
    secs = int(delta.total_seconds())
    if secs < 3600:
        return f"{secs // 60}m ago"
    if secs < 86400:
        return f"{secs // 3600}h ago"
    return f"{secs // 86400}d ago"


@cli.command("status")
def status(
    uri: Annotated[str, typer.Argument(help="URI or path to crawler YAML config file")],
    runs: Annotated[
        int, typer.Option("--runs", "-r", help="Number of recent runs to show")
    ] = 5,
):
    """Show crawler status: recent runs and stored document count."""
    configure_logging(level="ERROR")
    crawler = get_crawler(uri)

    console.print(f"\n[bold]{crawler.name}[/bold]")
    if crawler.description and crawler.description != crawler.name:
        console.print(f"[dim]{crawler.description}[/dim]\n")
    else:
        console.print()

    # Recent runs table
    recent = crawler.get_recent_runs(limit=runs)
    if recent:
        table = Table(title="Recent Runs")
        table.add_column("Run ID")
        table.add_column("Started")
        table.add_column("Age")

        now = datetime.now()
        for run_id, started in recent:
            age = format_age(now - started)
            table.add_row(run_id[:12], started.strftime("%Y-%m-%d %H:%M"), age)
        console.print(table)
    else:
        console.print("[yellow]No runs recorded[/yellow]")

    # Emit count
    emit_count = crawler.count_emits()
    console.print(f"\n[bold]Stored documents:[/bold] {emit_count:,}\n")


if __name__ == "__main__":
    cli()

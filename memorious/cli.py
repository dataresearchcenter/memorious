"""Memorious CLI - command line interface for crawler management."""

import sys
from pathlib import Path
from typing import Annotated, Optional

import typer
from anystore.logging import get_logger
from openaleph_procrastinate.app import run_sync_worker
from rich.console import Console

from memorious.core import get_settings, init_memorious, settings
from memorious.logic.crawler import get_crawler
from memorious.settings import VERSION
from memorious.tasks import app as procrastinate_app

log = get_logger(__name__)
console = Console(stderr=True)

cli = typer.Typer(no_args_is_help=True, pretty_exceptions_enable=settings.debug)


@cli.callback(invoke_without_command=True)
def main(
    version: Annotated[
        Optional[bool], typer.Option("--version", "-v", help="Show version")
    ] = False,
    settings: Annotated[
        Optional[bool], typer.Option(help="Show current settings")
    ] = False,
):
    """Crawler framework for documents and structured scrapers."""
    if version:
        console.print(f"memorious {VERSION}")
        raise typer.Exit()
    if settings:
        console.print(get_settings())

    init_memorious()


@cli.command("run")
def run_crawler(
    uri: Annotated[str, typer.Argument(help="URI or path to crawler YAML config file")],
    src: Annotated[
        Optional[Path],
        typer.Option(
            "--src",
            help="Directory containing custom modules to add to Python path",
            exists=True,
            file_okay=False,
            dir_okay=True,
            resolve_path=True,
        ),
    ] = None,
    continue_on_error: Annotated[
        bool,
        typer.Option(
            "--continue-on-error", help="Don't stop crawler execution on error"
        ),
    ] = False,
    flush: Annotated[
        bool,
        typer.Option("--flush", help="Delete all existing data before execution"),
    ] = False,
):
    """Run a crawler from a YAML config file."""
    crawler = get_crawler(uri)
    if src:
        sys.path.insert(0, str(src))
    if flush:
        crawler.flush()
    crawler.run(continue_on_error=continue_on_error)
    console.print(f"Crawler [bold]{crawler.name}[/bold] completed")


@cli.command("worker")
def worker(
    concurrency: Annotated[
        int, typer.Option("--concurrency", "-c", help="Number of concurrent workers")
    ] = 1,
):
    """Start the procrastinate worker to process crawler jobs."""
    console.print(f"Starting memorious worker with concurrency={concurrency}")
    run_sync_worker(procrastinate_app, concurrency=concurrency)


@cli.command("cancel")
def cancel(
    uri: Annotated[str, typer.Argument(help="URI or path to crawler YAML config file")],
):
    """Cancel execution of a crawler."""
    crawler = get_crawler(uri)
    crawler.cancel()
    console.print(f"Crawler [bold]{crawler.name}[/bold] cancelled")


@cli.command("flush")
def flush_crawler(
    uri: Annotated[str, typer.Argument(help="URI or path to crawler YAML config file")],
):
    """Delete all data and tags generated by a crawler."""
    crawler = get_crawler(uri)
    crawler.flush()
    console.print(f"Crawler [bold]{crawler.name}[/bold] data flushed")


if __name__ == "__main__":
    cli()

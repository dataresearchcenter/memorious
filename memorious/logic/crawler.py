"""Crawler orchestration and runtime management."""

import logging
from importlib import import_module
from pathlib import Path

from servicelayer.cache import make_key
from servicelayer.extensions import get_entry_point
from servicelayer.jobs import Dataset, Job

from memorious.core import conn, settings, tags
from memorious.model import Crawl, CrawlerConfig, CrawlerStage, Queue

log = logging.getLogger(__name__)


class Crawler:
    """A processing graph that constitutes a crawler.

    Wraps CrawlerConfig with runtime state and operations.
    """

    def __init__(self, manager, source_file: str | Path):
        self.manager = manager
        self.source_file = Path(source_file)

        # Load and parse config using anystore's from_yaml_uri
        self.config = CrawlerConfig.from_yaml_uri(self.source_file.as_uri())

        # Runtime state
        self.queue = Dataset(conn, self.name)

        # Build stage objects
        self.stages = {}
        for stage_name, stage_config in self.config.pipeline.items():
            self.stages[stage_name] = CrawlerStage(self, stage_name, stage_config)

    # Delegate to config properties
    @property
    def name(self) -> str:
        return self.config.name

    @property
    def description(self) -> str:
        return self.config.title or self.name

    @property
    def category(self) -> str | None:
        return self.config.category

    @property
    def init_stage(self) -> str:
        return self.config.init

    @property
    def delay(self) -> int:
        return self.config.delay

    @property
    def expire(self) -> int:
        """Expire time in seconds."""
        # Use config expire (in days) or fall back to settings
        expire_days = self.config.expire or settings.expire
        return expire_days * 86400

    @property
    def stealthy(self) -> bool:
        return self.config.stealthy

    @property
    def aggregator_config(self) -> dict:
        if self.config.aggregator:
            return {
                "method": self.config.aggregator.method,
                "params": self.config.aggregator.params,
            }
        return {}

    @property
    def aggregator_method(self):
        if not self.config.aggregator:
            return None

        method = self.config.aggregator.method
        # method A: via a named Python entry point
        func = get_entry_point("memorious.operations", method)
        if func is not None:
            return func
        # method B: direct import from a module
        if ":" in method:
            package, method_name = method.rsplit(":", 1)
            module = import_module(package)
            return getattr(module, method_name)
        raise ValueError(f"Unknown method: {method}")

    def aggregate(self, context):
        if self.aggregator_method:
            log.info("Running aggregator for %s", self.name)
            params = self.config.aggregator.params if self.config.aggregator else {}
            self.aggregator_method(context, params)

    def flush(self):
        """Delete all run-time data generated by this crawler."""
        self.queue.cancel()
        Crawl.flush(self)
        self.flush_tags()

    def flush_tags(self):
        tags.delete(prefix=make_key(self, "tag"))

    def cancel(self):
        Crawl.abort_all(self)
        self.queue.cancel()

    def run(self, incremental=None, run_id=None, continue_on_error=None):
        """Queue the execution of a particular crawler."""
        state = {
            "crawler": self.name,
            "run_id": run_id or Job.random_id(),
            "incremental": settings.incremental,
            "continue_on_error": settings.continue_on_error,
        }
        if incremental is not None:
            state["incremental"] = incremental
        if continue_on_error is not None:
            state["continue_on_error"] = continue_on_error

        # Cancel previous runs:
        self.cancel()
        init_stage = self.get(self.init_stage)
        Queue.queue(init_stage, state, {})

    @property
    def is_running(self) -> bool:
        """Is the crawler currently running?"""
        for job in self.queue.get_jobs():
            if not job.is_done():
                return True
        return False

    @property
    def last_run(self):
        return Crawl.last_run(self)

    @property
    def op_count(self):
        """Total operations performed for this crawler."""
        return Crawl.op_count(self)

    @property
    def runs(self):
        return Crawl.runs(self)

    @property
    def latest_runid(self):
        return Crawl.latest_runid(self)

    @property
    def pending(self):
        status = self.queue.get_status()
        return status.get("pending")

    def get(self, name: str) -> CrawlerStage | None:
        return self.stages.get(name)

    def __str__(self) -> str:
        return self.name

    def __iter__(self):
        return iter(self.stages.values())

    def __repr__(self) -> str:
        return f"<Crawler({self.name})>"

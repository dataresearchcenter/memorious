"""Crawler orchestration and runtime management."""

from importlib import import_module
from pathlib import Path

from anystore.logging import get_logger
from anystore.types import SDict
from anystore.util import ensure_uuid
from openaleph_procrastinate.manage import cancel_jobs
from openaleph_procrastinate.settings import OpenAlephSettings
from procrastinate.jobs import DeleteJobCondition
from servicelayer.extensions import get_entry_point

from memorious.core import get_tags, settings
from memorious.model import CrawlerConfig, CrawlerStage

log = get_logger(__name__)


class Crawler:
    """A processing graph that constitutes a crawler.

    Wraps CrawlerConfig with runtime state and operations.
    """

    def __init__(self, manager, source_file: str | Path):
        self.manager = manager
        self.source_file = Path(source_file)

        # Load and parse config using anystore's from_yaml_uri
        self.config = CrawlerConfig.from_yaml_uri(self.source_file)

        # Build stage objects
        self.stages = {}
        for stage_name, stage_config in self.config.pipeline.items():
            self.stages[stage_name] = CrawlerStage(self, stage_name, stage_config)

    # Delegate to config properties
    @property
    def name(self) -> str:
        return self.config.name

    @property
    def description(self) -> str:
        return self.config.title or self.name

    @property
    def init_stage(self) -> str:
        return self.config.init

    @property
    def delay(self) -> int:
        return self.config.delay

    @property
    def expire(self) -> int:
        """Expire time in seconds."""
        expire_days = self.config.expire or settings.expire
        return expire_days * 86400

    @property
    def stealthy(self) -> bool:
        return self.config.stealthy

    @property
    def aggregator_config(self) -> dict:
        if self.config.aggregator:
            return {
                "method": self.config.aggregator.method,
                "params": self.config.aggregator.params,
            }
        return {}

    @property
    def aggregator_method(self):
        if not self.config.aggregator:
            return None

        method = self.config.aggregator.method
        func = get_entry_point("memorious.operations", method)
        if func is not None:
            return func
        if ":" in method:
            package, method_name = method.rsplit(":", 1)
            module = import_module(package)
            return getattr(module, method_name)
        raise ValueError(f"Unknown method: {method}")

    def aggregate(self, context):
        if self.aggregator_method:
            log.info("Running aggregator", dataset=self.name)
            params = self.config.aggregator.params if self.config.aggregator else {}
            self.aggregator_method(context, params)

    def flush(self):
        """Delete all run-time data generated by this crawler."""
        self.flush_tags()

    def flush_tags(self):
        tags = get_tags(self.name)
        tags.delete(prefix=self.name)

    def cancel(self):
        """Cancel all pending/running jobs for this crawler."""

        # cancel_jobs requires a real database, skip in testing mode
        if not OpenAlephSettings().in_memory_db:
            cancel_jobs(dataset=self.name)

    def start(self, incremental=None, run_id=None, continue_on_error=None):
        """
        Queue the initial stage for execution by external workers.

        This only defers the first task - external workers must be running
        to pick up and process the jobs.
        """

        # Cancel previous runs
        self.cancel()

        # Defer the initial stage
        self.defer(
            self.config.init,
            data={},
            incremental=incremental,
            run_id=run_id,
            continue_on_error=continue_on_error,
        )

    def run(self, incremental=None, run_id=None, continue_on_error=None):
        """
        Run the crawler synchronously.

        Defers the initial stage and starts a worker to process all jobs
        until completion.
        """
        from memorious.tasks import app

        # Defer the initial stage
        self.start(
            incremental=incremental,
            run_id=run_id,
            continue_on_error=continue_on_error,
        )

        # Run worker until all jobs are processed
        app.run_worker(wait=False, delete_jobs=DeleteJobCondition.SUCCESSFUL)

    def defer(
        self,
        stage: str,
        data: SDict,
        incremental=None,
        run_id=None,
        continue_on_error=None,
    ):
        """
        Emit (defer) a new job for this crawler
        """
        # Import here to avoid circular import with tasks.py
        from memorious.tasks import defer_stage

        run_id = run_id or ensure_uuid()
        incr = incremental if incremental is not None else settings.incremental
        coe = (
            continue_on_error
            if continue_on_error is not None
            else settings.continue_on_error
        )
        defer_stage(
            dataset=self.name,
            stage=stage,
            run_id=run_id,
            config_file=str(self.source_file),
            data=data,
            incremental=incr,
            continue_on_error=coe,
        )

    def get(self, name: str) -> CrawlerStage | None:
        return self.stages.get(name)

    def __str__(self) -> str:
        return self.name

    def __iter__(self):
        return iter(self.stages.values())

    def __repr__(self) -> str:
        return f"<Crawler({self.name})>"
